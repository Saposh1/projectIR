{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ue93XSh6_SDO"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-1795  GCE       4                                       RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security → Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"k_9DWXuPAI-C"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"kEBMGqP8AKi8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import math\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iksVOm-tAOAH"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan  4 19:38 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aG5evy9hAP4i"},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MDgdtab8ASUW"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-1795-m.c.irproj-372318.internal:46521\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2869cb5520>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"Z8371eIHAVUt"},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'irproject2' \n","\n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"QDccMmzeBw-5"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","# doc_title_pairs = parquetFile.select(\"title\",\"id\").rdd\n","# doc_anchor_pairs = parquetFile.select(\"anchor_text\").rdd"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"e4fNuOZ2CEB4"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"ltod0E2jCGmn"},"outputs":[{"name":"stderr","output_type":"stream","text":["23/01/04 21:03:18 WARN org.apache.spark.SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"]}],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"KhOYMhYPCKX9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-PLhZXEnCPij"},"outputs":[],"source":["# def anchor_change(doc_anchor):\n","#   edge_set = []\n","#   for id, anchor in doc_anchor[0]:\n","#     if (id, anchor) in edge_set:\n","#       continue\n","#     else:\n","#       edge_set.append((id, anchor))\n","#   return edge_set\n","\n","# doc_anchor_pairs_new = doc_anchor_pairs.flatMap(anchor_change).distinct().groupByKey().mapValues(list).map(lambda x:(\" \".join(x[1]),x[0]))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Lk121j_hCv1v"},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords =[\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","#remember to change NUM_BUCKETS back to 124 \n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def tokenize(text):\n","    \"\"\"\n","    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n","    \n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]    \n","    return list_of_tokens\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  # YOUR CODE HERE\n","  return([(k,(id,v)) for k,v in Counter(tokens).items() if k not in all_stopwords and v>0])\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda tup: tup[0])\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda x:(x[0],len(x[1])))\n","\n","def partition_postings_and_write(postings):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  dict1 = postings.map(lambda token: (token2bucket_id(token[0]), token)).groupByKey()\n","  return dict1.map(lambda x: InvertedIndex.write_a_posting_list(x,bucket_name))\n","\n","def calc_dl(tokens,doc_id):\n","  return (doc_id,len(tokens))\n","\n","def calc_total_term(postings):\n","  return postings.mapValues(helpFunc).collectAsMap()\n","\n","def helpFunc(posting):\n","  count=0\n","  for tup in posting:\n","    count+=tup[1]\n","  return count\n","\n","def doc_norm(text,doc_id):\n","  tokens=tokenize(text)\n","  dict_tokens=Counter(tokens)\n","  sum=0\n","  for term, tf in dict_tokens.items():\n","    sum+=tf*tf\n","  return (doc_id,math.sqrt(sum))\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"x1Lcc8GIDP32"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘body_index’: File exists\r\n","mkdir: cannot create directory ‘title_index’: File exists\r\n","mkdir: cannot create directory ‘anchor_index’: File exists\r\n"]}],"source":["min_body=50\n","!mkdir body_index title_index anchor_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDzrjAx9DTys"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# #body\n","word_count_bodys = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","posting_lists_body = word_count_bodys.groupByKey().mapValues(reduce_word_counts)\n","filter_bodys = posting_lists_body.filter(lambda x: len(x[1])>min_body)\n","#global statistics\n","body_df=calculate_df(filter_bodys).collectAsMap()\n","posting_locs_body = partition_postings_and_write(filter_bodys).collect()\n","body_total_term=calc_total_term(filter_bodys)\n","body_dl=doc_text_pairs.map(lambda x: calc_dl(tokenize(x[0]),x[1])).collectAsMap()\n","body_norm=doc_text_pairs.map(lambda x: doc_norm(x[0],x[1])).collectAsMap()\n","super_posting_locs_body = defaultdict(list)\n","\n","#title\n","# word_count_titles = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","# posting_lists_title = word_count_titles.groupByKey().mapValues(reduce_word_counts)\n","# #global statistics\n","# title_df=calculate_df(posting_lists_title).collectAsMap()\n","# posting_locs_title = partition_postings_and_write(posting_lists_title).collect()\n","# title_total_term=calc_total_term(posting_lists_title)\n","# title_dl=doc_title_pairs.map(lambda x: calc_dl(tokenize(x[0]),x[1])).collectAsMap()\n","# title_norm=doc_title_pairs.map(lambda x: doc_norm(x[0],x[1])).collectAsMap()\n","# super_posting_locs_title = defaultdict(list)\n","\n","# # #Anchor\n","# word_count_anchors = doc_anchor_pairs_new.flatMap(lambda x: word_count(x[0], x[1]))\n","# posting_lists_anchor = word_count_anchors.groupByKey().mapValues(reduce_word_counts)\n","# #global statistics\n","# anchor_df=calculate_df(posting_lists_anchor).collectAsMap()\n","# posting_locs_anchor = partition_postings_and_write(posting_lists_anchor).collect()\n","# anchor_total_term=calc_total_term(posting_lists_anchor)\n","# anchor_dl=doc_anchor_pairs_new.map(lambda x: calc_dl(tokenize(x[0]),x[1])).collectAsMap()\n","# anchor_norm=doc_anchor_pairs_new.map(lambda x: doc_norm(x[0],x[1])).collectAsMap()\n","# super_posting_locs_anchor = defaultdict(list)\n","\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"ZUfKtYM2Eajn","outputId":"f6d1cbc8-55f3-4f11-ef6b-8d0884adea98"},"outputs":[],"source":["#body\n","for blob in client.list_blobs(bucket_name, prefix='postings_gcp_body'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs_body[k].extend(v)\n","\n","#title\n","# for blob in client.list_blobs(bucket_name, prefix='postings_gcp/title'):\n","#   if not blob.name.endswith(\"pickle\"):\n","#     continue\n","#   with blob.open(\"rb\") as f:\n","#     posting_locs = pickle.load(f)\n","#     for k, v in posting_locs.items():\n","#       super_posting_locs_title[k].extend(v)\n","\n","#anchor\n","# for blob in client.list_blobs(bucket_name, prefix='postings_gcp_anchor'):\n","#   if not blob.name.endswith(\"pickle\"):\n","#     continue\n","#   with blob.open(\"rb\") as f:\n","#     posting_locs = pickle.load(f)\n","#     for k, v in posting_locs.items():\n","#       super_posting_locs_anchor[k].extend(v)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"FY21DZRuFIO6"},"outputs":[],"source":["#body\n","from inverted_index_gcp import InvertedIndex\n","index_body=InvertedIndex()\n","index_body.posting_locs=super_posting_locs_body\n","index_body.df=body_df\n","index_body.DL=body_dl\n","index_body.norma=body_norm\n","index_body.term_total=body_total_term\n","\n","#title\n","# index_title=InvertedIndex()\n","# index_title.posting_locs=super_posting_locs_title\n","# index_title.df=title_df\n","# index_title.DL=title_dl\n","# index_title.norma=title_norm\n","# index_title.term_total=title_total_term\n","\n","# # #anchor\n","# index_anchor=InvertedIndex()\n","# index_anchor.posting_locs=super_posting_locs_anchor\n","# index_anchor.df=anchor_df\n","# index_anchor.DL=anchor_dl\n","# index_anchor.norma=anchor_norm\n","# index_anchor.term_total=anchor_total_term\n","\n","index_body.write_index('.', 'index_body')\n","# index_anchor.write_index('.', 'index_anchor')\n","# index_title.write_index('.', 'index_title')"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Sr5ewiNlGFu7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://index_body.pkl [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run          \n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","/ [1 files][152.2 MiB/152.2 MiB]                                                \n","Operation completed over 1 objects/152.2 MiB.                                    \n"]}],"source":["# upload to gs\n","index_src = \"index_body.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp_body/{index_src}'\n","!gsutil cp $index_src $index_dst\n","\n","# index_src = \"index_anchor.pkl\"\n","# index_dst = f'gs://{bucket_name}/postings_gcp_body/{index_src}'\n","# !gsutil cp $index_src $index_dst\n","\n","# index_src = \"index_title.pkl\"\n","# index_dst = f'gs://{bucket_name}/postings_gcp/title/{index_src}'\n","# !gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"w2zXgeglGMYx"},"outputs":[{"name":"stdout","output_type":"stream","text":["152.17 MiB  2023-01-04T22:00:25Z  gs://irproject2/postings_gcp_body/index_body.pkl\r\n","TOTAL: 1 objects, 159562422 bytes (152.17 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":1}